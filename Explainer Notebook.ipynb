{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainer Notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivation\n",
    "What is your dataset?\n",
    "\n",
    "Why did you choose this/these particular dataset(s)?\n",
    "\n",
    "What was your goal for the end user’s experience?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset consists of animal pages on Wikipedia, where we used wescraping methods and the WikiData API to collect a dataset.\n",
    "We choose this particular dataset because when looking at Wikipedia in general there is always a link to another page within a wikipedia page and then another link. This we thought would be interesting to look at in regards to the kingdom of animals. Because some groups of animals are more likely to mention each other in a wikipedia article. We first analyzed all of the approxomately 30k wikipedia pages and then narrowed down our network to only reptiles in order to narrow down the dataset.\n",
    "Our goal with the analysis of our Wikipedia animal dataset was to show the user and viewer how contected these animals actually are on wikipedia."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic stats\n",
    "Write about your choices in data cleaning and preprocessing\n",
    "\n",
    "Write a short section that discusses the dataset stats (here you can recycle the work you did for Project Assignment A)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that will be used in the analysis, has been gathered by using the WikiData API using their query builder. Due to the Wikipedia pages not having a category called “Animals” that allows you to gather all the animal pages, we will use an alternative which is querying on the Wikidata pages that have a “Animal Diversity Website (ADW) taxon id”. The query to gather the data is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    SELECT DISTINCT ?item ?itemLabel WHERE {\n",
    "      SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE]\". }\n",
    "      {\n",
    "        SELECT DISTINCT ?item WHERE {\n",
    "          ?item p:P4024 ?statement0.\n",
    "          ?statement0 (ps:P4024) _:anyValueP4024.\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    '''\n",
    "query_reptile = '''\n",
    "    SELECT DISTINCT ?item ?itemLabel WHERE {\n",
    "      SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE]\". }\n",
    "      {\n",
    "        SELECT DISTINCT ?item WHERE {\n",
    "          ?item p:P4024 ?statement0.\n",
    "          ?statement0 (ps:P4024) _:anyValueP4024.\n",
    "          ?item p:P5473 ?statement1.\n",
    "          ?statement1 (ps:P5473) _:anyValueP5473.\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from get_links import links_on_page\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from netwulf import visualize\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first query just request all the pages where it has a Animal Diversity Website (ADW) taxon id, this query is made by their auto query builder.\n",
    "The second query request all the pages where it has a Animal Diversity Website (ADW) taxon id aswell as a Reptile Database ID.\n",
    "\n",
    "The get_wiki_links function takes n amount of wikidata ids, and in the query they are seperated with | as this allows us to query up to 50 ids at a time. We then loop over the response and check if it contains a link to an english wikipedia site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_links(item_ids):\n",
    "    url = f\"https://www.wikidata.org/w/api.php?action=wbgetentities&ids={'|'.join(item_ids)}&props=sitelinks/urls&format=json&sitefilter=enwiki\"\n",
    "    response = requests.get(url)\n",
    "    data = json.loads(response.text)\n",
    "    wiki_links = []\n",
    "    for item_id in item_ids:\n",
    "        wikipedia_url = data[\"entities\"][item_id][\"sitelinks\"]\n",
    "        if \"enwiki\" in wikipedia_url: # only save if the page has a reference to an english wikipage\n",
    "            link = wikipedia_url[\"enwiki\"][\"url\"]\n",
    "            wiki_links.append(link)\n",
    "    return wiki_links"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the actual quering, we start by getting all the wikidata pages that fit our query (query & query_reptile). We loop over 50 pages at a time, and get their wikipedia links with get_wiki_links, and write the results to a .txt file where each entry is seperated by a new line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://query.wikidata.org/sparql'\n",
    "r = requests.get(url, params = {'format': 'json', 'query': query_reptile})\n",
    "data = r.json()\n",
    "\n",
    "data = data[\"results\"][\"bindings\"]\n",
    "temp = []\n",
    "for entries in tqdm(range(0,len(data),50)):\n",
    "    if entries+50 < len(data):\n",
    "        sub_list = [ids[\"itemLabel\"][\"value\"] for ids in data[entries:(entries+50)]]\n",
    "        temp = temp + get_wiki_links(sub_list)\n",
    "    else:\n",
    "        sub_list = [ids[\"itemLabel\"][\"value\"] for ids in data[entries:(len(data))]]\n",
    "        temp = temp + get_wiki_links(sub_list)\n",
    "file = open('animal_links_reptile.txt','w')\n",
    "for item in temp:\n",
    "    file.write(item+\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for creating our nodes and edges. We open the .txt file from before, and prepare for the web scraping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist_weights = {}\n",
    "edgelist_weights_long = {}\n",
    "#animal_name = \"Elephant\"\n",
    "names = names_from_table() # Gets all the entries in the larger table on https://en.wikipedia.org/wiki/List_of_animal_names\n",
    "names_long = {}\n",
    "with open('data/animal_links_reptile.txt', 'r') as f:\n",
    "    entries = f.read().splitlines()\n",
    "for name in tqdm(entries):\n",
    "    name_temp = name.split(\"/\")[-1]\n",
    "    names_long[name_temp] = 0 # Saving all the entries in the txt file in a dict, for a fast comparisons (ie. only make pairs\n",
    "                                #  with animals and not wikipages for unrelated stuff)\n",
    "attributes_dict = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each wikipedia link in the .txt file, we get all the links on the page aswell as look for infoboxes using links_on_page. This function just uses the wikipedia api to get all the information on the page in json format, and then we find all href in the wikipedia page and save these.\n",
    "Then we look for a infobox, since there are 2 ways wikipedia makes these, we have to check if infobox is none, as this indicates the infobox is the other type.\n",
    "Then we go through the infobox (table) and extract predefined values we wanna use as attributes.\n",
    "In case there is no infobox on the page, ie. infobox is never updated from having None as values, we discard this page as it is most likely a redirect page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def links_on_page(animal_name=\"Elephant\"):\n",
    "    url = \"https://en.wikipedia.org/w/api.php?action=parse&page=\"+animal_name+\"&format=json\" # we can either put the title page or the URL version\n",
    "                                                                                             # of the title, ex. Malayan softshell turtle = Malayan_softshell_turtle\n",
    "    response = requests.get(url)\n",
    "    html = json.loads(response.content.decode('utf-8'))['parse']['text']['*'] # Default way the result comes in\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    links = soup.find_all('a', href=lambda href: href and href.startswith('/wiki/') and not href.endswith('.jpg') and not href.endswith('.png'))\n",
    "    # ^ sometimes the links link to jpg and png file, sort them out\n",
    "    result = []\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        title = link.get('title')\n",
    "        text = link.text\n",
    "        result.append(href)\n",
    "    info = {\"Class:\": None, \"Order:\":None, \"Superfamily:\":None, \"Family:\":None,\"Name:\": None} # Default dict allows us to see if its redirect later\n",
    "    infobox = soup.find('table', {'class': 'infobox biota biota-infobox'}) # One type of infoboxes wiki uses\n",
    "    if infobox is None:\n",
    "        infobox = soup.find('table', {'class': 'infobox biota'}) # Other type of infoboxes wiki uses\n",
    "    if infobox:\n",
    "        rows = infobox.find_all('tr') # Going through the rows in the infobox\n",
    "        for row in rows:\n",
    "            td = row.find_all('td')\n",
    "            if td: # Checking if empty\n",
    "                if td[0].text.strip() in [\"Class:\", \"Order:\", \"Superfamily:\", \"Family:\"]: # the info we want is stored in a row at a time\n",
    "                                                                                          # it has two columns first being Class, Order...\n",
    "                                                                                          # the other being the attributes we will save \n",
    "                    if td:\n",
    "                        info[td[0].text.strip()] = td[1].text.strip() # Making the category the key, and the attribute value the value\n",
    "        info[\"Name:\"] = animal_name # Updating from default \n",
    "\n",
    "    return result, info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the graph we make is the one where we only make a edge a list of 224 animal names from https://en.wikipedia.org/wiki/List_of_animal_names, this wikipedia page contains two tables, one of them being one with the overall animal species (no subspecies).\n",
    "Due to it being a wikipedia page, there is a lot of references and stuff we dont want in our scraping which is removed with re.sub, where we have given it some predefined things to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names_from_table():\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_animal_names\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    html_content = response.content\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find table on the page\n",
    "    table = soup.find_all('table',{\"class\":\"wikitable\"})[1]\n",
    "    \n",
    "    # Find all the rows in the table\n",
    "    t_rows = table.find_all('tr')\n",
    "    \n",
    "    ths = t_rows[0].find_all('th')\n",
    "    \n",
    "    header = [th.text.replace(\"\\n\",\"\") for th in ths]\n",
    "    \n",
    "    rows = []\n",
    "    names = {}\n",
    "    for tr in t_rows[1:]:\n",
    "        tds = tr.find_all('td')\n",
    "        if tds:\n",
    "            links = tds[0].find_all('a')\n",
    "            if links:\n",
    "                row = [td.text.replace(\"\\n\",\"\") for td in tds]\n",
    "                rows.append(row)\n",
    "                cleaned_row = re.sub(r'\\(.*\\)|Also see.*|\\[\\d+\\]|See.*', '', row[0]) # The table is not clean, many unwanted formating we remove here\n",
    "                for link in links:\n",
    "                    link_href = link.get('href')\n",
    "                    if link_href.startswith('/wiki/'):\n",
    "                        names[link_href] = cleaned_row\n",
    "                        break\n",
    "    import pandas as pd\n",
    "    df_animals = pd.DataFrame(rows, columns=header)\n",
    "\n",
    "    return names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the actual scraping of websites, we loop over all the entries from our .txt file. We check if its redirect by looking at if infobox is None (its default scenario) if not, we add it to our edgelist. One of them to be used for directed and the other for undirected graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in tqdm(entries):\n",
    "    temp_string = name.split(\"/\")[-1]\n",
    "    result, info = links_on_page(animal_name=temp_string)\n",
    "    if info[\"Name:\"] is not None: # A way to remove wiki redirects from the final result, as redirects dont have the infoboxes we want\n",
    "        attributes_dict[info[\"Name:\"]] = info # Making nested dicts to quickly get attributes later\n",
    "        for entry in result:\n",
    "            if entry in names: # making one graph where we only make edges to the table from https://en.wikipedia.org/wiki/List_of_animal_names\n",
    "                pair = (\"/wiki/\"+temp_string,entry) # Making pairs to compare for the dict\n",
    "                pair_inverted = (entry,\"/wiki/\"+temp_string)\n",
    "                if pair in edgelist_weights:\n",
    "                    edgelist_weights[pair] += 1 # If the pair is already in the dict, the weight is increased\n",
    "                elif pair_inverted in edgelist_weights:\n",
    "                    edgelist_weights[pair_inverted] += 1 # If the inverted pair is already in the dict, the weight is increased\n",
    "                else:\n",
    "                    edgelist_weights[pair] = 1 # If the pair is not in the dict, the weight is 1\n",
    "            if entry.split(\"/\")[-1] in names_long: # making other graph where we make edges between entries from the .txt file\n",
    "                pair = (temp_string, entry.split(\"/\")[-1]) # Only taking the last part of the URL (the URL title of the page)\n",
    "                if pair in edgelist_weights_long:\n",
    "                    edgelist_weights_long[pair] += 1 # If the pair is already in the dict, the weight is increased\n",
    "                else:\n",
    "                    edgelist_weights_long[pair] = 1 # If the pair is not in the dict, the weight is 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid doing this over and over we dump the results as pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/data_plain_reptile_test.pickle', 'wb') as fp:\n",
    "    pickle.dump(edgelist_weights, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/data_plain_long_reptile_test.pickle', 'wb') as fp:\n",
    "    pickle.dump(edgelist_weights_long, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/Reptile_attributes.pickle', 'wb') as fp:\n",
    "    pickle.dump(attributes_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visulize them, they are first loaded and then we loop over all the entries and add them to an edgelist, and construct the graph with this edgelist.\n",
    "We then add all our attributes to the graph, and for sanity check we remove nodes than dont have attributes saved in our .pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/all_animal_to_all_animal.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "with open('data/animal_attributes.pickle', 'rb') as handle:\n",
    "    c = pickle.load(handle)\n",
    "\n",
    "values = list(b.values())\n",
    "plt.hist(values, bins=np.arange(max(values))-0.5, edgecolor='black')\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(range(1, max(values) + 1))\n",
    "plt.xlabel('Number of references')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "edgelist = [None]*len(b)\n",
    "for i,items in enumerate(b):\n",
    "    edgelist[i] = (items[0].replace(\"/wiki/\", \"\"),items[1].replace(\"/wiki/\", \"\"),int(b[items]))\n",
    "G = nx.DiGraph()\n",
    "\n",
    "G.add_weighted_edges_from(edgelist)\n",
    "print(G)\n",
    "\n",
    "to_remove =[]\n",
    "for Names in tqdm(G.nodes):\n",
    "    if Names in c:\n",
    "        G.nodes[Names]['Class'], G.nodes[Names]['Order'], G.nodes[Names]['Superfamily'], G.nodes[Names]['Family'], _ = c[Names].values()\n",
    "    else:\n",
    "        to_remove.append(Names) # Some nodes get added to graph even though they are redirects, the cause is known but no good way to handle it\n",
    "for names in to_remove:\n",
    "    G.remove_node(names)\n",
    "print(G)\n",
    "network, config = visualize(G)\n",
    "\n",
    "degree = []\n",
    "for node in G.nodes():\n",
    "    degree.append(G.degree(node))\n",
    "plt.hist(degree,bins=np.arange(max(values))-0.5, edgecolor='black')\n",
    "plt.title(\"Degree distribution\")\n",
    "plt.xlabel('Number of degrees')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tools, theory and analysis. Describe the process of theory to insight\n",
    "Describe which network science tools and data analysis strategies you’ve used, how those network science measures work, and why the tools you’ve chosen are right for the problem you’re solving.\n",
    "\n",
    "Talk about how you’ve worked with text, including regular expressions, unicode, etc.\n",
    "\n",
    "How did you use the tools to understand your dataset?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network theory\n",
    "\n",
    "Fraction of edges\n",
    "\n",
    "Assortivity\n",
    "\n",
    "Modularity\n",
    "\n",
    "$M=\\sum_{c=1}^{n_c}\\left[\\frac{L_c}{L}-(\\frac{k_c}{2L})^2\\right]$ \n",
    "\n",
    "##### Louvain\n",
    "The Louvain algorithm is a community detection algorithm used to identify the communities or groups within a network.\n",
    "\n",
    "The algorithm works by optimizing a modularity function that measures the quality of the community structure. The modularity function quantifies the extent to which the number of edges within communities is higher than the expected number in a random network with the same degree sequence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening the stored data with two helper functions, clean_family cleans the attribute values from wikipedia as these can contain symbols and extra stuff we are not interested in. The add_attr function adds the attributes to the network.\n",
    "\n",
    "The banned set, is for cleaning up the nodes for the \"cleaned\" network described in https://natasha0301.github.io/Project-website/network-analysis/\n",
    "\n",
    "Please note some of the functions used below, are either directly from previous assignments or modified to fit the problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pickle\n",
    "import random\n",
    "import community\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import colorsys\n",
    "from netwulf import visualize\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def clean_family(string):\n",
    "    if string is None:\n",
    "        return None\n",
    "    string = string.replace(\"†\",\"\").replace(\" (\",\"\")\n",
    "    count = 0\n",
    "    for i,chara in enumerate(string):\n",
    "        if chara.isupper():\n",
    "            count += 1\n",
    "            if count > 1:\n",
    "                return string[:i]\n",
    "    return string\n",
    "\n",
    "def add_attr(Graph,attr_dict):\n",
    "    to_remove =[]\n",
    "    for Names in Graph.nodes:\n",
    "        if Names in attr_dict:\n",
    "            Graph.nodes[Names]['Class'], Graph.nodes[Names]['Order'], Graph.nodes[Names]['Superfamily'], Graph.nodes[Names]['Family'], _ = attr_dict[Names].values()\n",
    "            Graph.nodes[Names]['Class'] = clean_family(Graph.nodes[Names]['Class']) if Graph.nodes[Names]['Class'] is not None else None\n",
    "        else:\n",
    "            to_remove.append(Names) # Some nodes get added to graph even though they are redirects, the cause is known but no good way to handle it\n",
    "    for names in to_remove:\n",
    "        Graph.remove_node(names)\n",
    "    return Graph\n",
    "\n",
    "# Load Network\n",
    "with open('data/all_animal_to_all_animal.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "with open('data/animal_attributes.pickle', 'rb') as handle:\n",
    "    c = pickle.load(handle)\n",
    "banned_set = {\"Animal\",\"Reptile\",\"Arthropod\",\"Chordate\",\"Bird\",\"Rodent\",\"Insect\"}\n",
    "for items in c:\n",
    "    banned_set.add(clean_family(c[items][\"Class:\"]))\n",
    "    banned_set.add(clean_family(c[items][\"Order:\"]))\n",
    "    banned_set.add(clean_family(c[items][\"Superfamily:\"]))\n",
    "    banned_set.add(clean_family(c[items][\"Family:\"]))\n",
    "#edgelist = [None]*len(b)\n",
    "banned_set = {}\n",
    "edgelist = []\n",
    "for i,items in enumerate(b):\n",
    "    if items[0] not in banned_set and items[1] not in banned_set:\n",
    "        edgelist.append([items[0].replace(\"/wiki/\", \"\"),items[1].replace(\"/wiki/\", \"\"),int(b[items])])\n",
    "G_reptile = nx.Graph()\n",
    "\n",
    "\n",
    "G_reptile.add_weighted_edges_from(edgelist)\n",
    "print(G_reptile)\n",
    "G_reptile_attr = add_attr(G_reptile,c)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the functions to calculate the fraction of edges in the same class or Family. The functions are not smart, ie. have to manully change between Class and Family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_class(G):\n",
    "    same_class_fractions = []\n",
    "    for node in G.nodes():\n",
    "        same_class_fractions = 0\n",
    "        total_neighbors = 0\n",
    "        \n",
    "        for neighbor in G.neighbors(node):\n",
    "            if G.nodes[neighbor][\"Class\"] == G.nodes[node][\"Class\"]:\n",
    "                same_class_fractions += 1\n",
    "            total_neighbors += 1\n",
    "        \n",
    "        if total_neighbors > 0:\n",
    "            same_field_fraction = same_class_fractions / total_neighbors\n",
    "        else:\n",
    "            same_field_fraction = 0\n",
    "        same_class_fractions.append(same_field_fraction)\n",
    "    return same_class_fractions\n",
    "\n",
    "def same_class_rand(Graph):\n",
    "    shuffled_G = Graph.copy()\n",
    "    Family = [Graph.nodes[node][\"Class\"] for node in Graph.nodes()]\n",
    "    random.shuffle(Family)\n",
    "\n",
    "    for i, node in enumerate(shuffled_G.nodes()):\n",
    "        shuffled_G.nodes[node][\"Class\"] = Family[i]\n",
    "    return shuffled_G\n",
    "\n",
    "def same_class_rand_n(Graph, n = 250):\n",
    "    results = []\n",
    "    Family = [Graph.nodes[node][\"Class\"] for node in Graph.nodes()]\n",
    "    for i in range(n):\n",
    "        temp = same_class_rand(Graph)\n",
    "        results.append(np.mean(same_class(temp)))\n",
    "    plt.hist(results,label = \"Random\", bins = 20)\n",
    "    plt.xlabel(\"Fraction\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(\"Fraction of edges in same Class\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the size of the network we are not able to do a random shuffles between the edges, in a timely manner (network has >500,000 edges)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same class fractions\n",
    "same_class_fractions = same_class(G_reptile_attr)\n",
    "print(f\"The average fraction is: {np.mean(same_class_fractions):.3f}\")\n",
    "\n",
    "# Same class fractions for random graph\n",
    "same_class_fractions_rand = same_class_rand(G_reptile_attr)\n",
    "print(f\"The average fraction is: {np.mean(same_class(same_class_fractions_rand)):.3f}\")\n",
    "\n",
    "# Same class fractions\n",
    "#same_family_rand_n(G_reptile_attr, n = 250)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the assortativity coefficient, we reuse function from last assignment. This metric is used to tell  how likely similar nodes are to connect to other similar nodes over dissimilar nodes. In other words it reflects the degree of homophily or heterophily in the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assortative_matrix(Graph,unique_labels):\n",
    "    matrix = np.zeros((len(unique_labels),len(unique_labels)))\n",
    "\n",
    "    values = nx.get_node_attributes(Graph, \"Class\").values()\n",
    "    num_values = len(values)\n",
    "\n",
    "    for start, end in Graph.edges(): # Looping over all edges in graph (since its undirected its not really start and stop)\n",
    "        x = Graph.nodes[start][\"Class\"] # Getting the start point of the edge \n",
    "        y = Graph.nodes[end][\"Class\"] # Getting the end point of the edge \n",
    "        if x in unique_labels: \n",
    "            x = unique_labels[x]\n",
    "        else:\n",
    "            x = unique_labels[None] # in case x is nan\n",
    "            \n",
    "        if y in unique_labels:\n",
    "            y = unique_labels[y]\n",
    "        else:\n",
    "            y = unique_labels[None] # in case y is nan\n",
    "        matrix[x, y] += 1 \n",
    "        \n",
    "    num_edges = len(Graph.edges())\n",
    "    matrix /= num_edges # averaging the occurence with the total edges\n",
    "\n",
    "    trace = np.trace(matrix) # trace of the matrix, the sum of the diagonal entries\n",
    "    mix_matrix = np.sum(np.matmul(matrix, matrix))\n",
    "\n",
    "    r1 = (trace-mix_matrix)/(1-mix_matrix) # Eq. 2\n",
    "\n",
    "    return r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assortativity coefficient \n",
    "labels = np.unique([G_reptile_attr.nodes[node][\"Class\"] for node in G_reptile_attr.nodes() if G_reptile_attr.nodes[node][\"Class\"] is not None])\n",
    "unique_labels = {name: i for i,name in enumerate(labels)}\n",
    "unique_labels[None] = len(unique_labels)\n",
    "r1 = assortative_matrix(G_reptile_attr,unique_labels)\n",
    "print(f\"Assortativity coefficient: {r1:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compute modularity function is unchanged from last assigment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_modularity(Graph,partitioning):\n",
    "    L = Graph.number_of_edges()\n",
    "    M = 0\n",
    "    communities = set(partitioning.values())\n",
    "\n",
    "    for community in communities:\n",
    "        nodes_in_community = [node for node, community_id in partitioning.items() if community_id == community]\n",
    "        subgraph = Graph.subgraph(nodes_in_community)\n",
    "        k_c = sum(dict(subgraph.degree()).values())\n",
    "        L_c = subgraph.number_of_edges()\n",
    "        M += L_c / L - (k_c / (2 * L)) ** 2\n",
    "\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modularity Family split of Reptile Graph\n",
    "Family_split = nx.get_node_attributes(G_reptile_attr, \"Class\")\n",
    "\n",
    "#Then we compute the modularity of the partitioning by using the function above\n",
    "modularity = compute_modularity(G_reptile_attr, Family_split)\n",
    "\n",
    "print(f\"The modularity of the family split partitioning is {modularity:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modularity is used to see the degree of clustering and how well the nodes can be organized into distinct communities. This measure, measures the identification of meaningful substructures within the network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the size of the network it is not feasible to construct new random graphs, to determine if they are statistically different from zero, however the function to do this for smaller networks is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_edge_swap(GG,N):\n",
    "    \"\"\"\n",
    "    Example usage: G_new = double_edge_swap(G_reptile,len(list(G_reptile.edges())))\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    G_copy = GG.copy()\n",
    "    while(i<2*N):\n",
    "        edges = list(G_copy.edges()) # update edges after adding and removal\n",
    "        (u, v), (x, y) = random.sample(edges, 2) # picking two random edges\n",
    "        if (u != v) and (v != x) and (u, y) not in G_copy.edges() and (x, v) not in G_copy.edges(): # checking conditions\n",
    "            G_copy.add_edge(u, y)\n",
    "            G_copy.add_edge(x, v)\n",
    "            G_copy.remove_edge(u, v)\n",
    "            G_copy.remove_edge(x, y)\n",
    "            i+=1\n",
    "    return G_copy\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the louvain community detection algoritm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Louvain algorithm\n",
    "partition = community.best_partition(G_reptile,random_state=42) # dictionary, keys are the nodes and values are communities for each node\n",
    "\n",
    "# modularity of partition\n",
    "modularity = community.modularity(partition, G_reptile) \n",
    "\n",
    "size = []\n",
    "for community_ in set(partition.values()):\n",
    "    temp = []\n",
    "    for part in partition:\n",
    "        if partition[part] == community_:\n",
    "            temp.append(part)\n",
    "    size.append(len(temp))\n",
    "\n",
    "print(\"Number of communities:\", len(set(partition.values())))\n",
    "print(\"Community sizes:\", sorted(size,reverse=True))\n",
    "print(f\"Modularity: {modularity:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the performance we will find the accuracy aswell as construct a confusion matrix. We start by getting the partion done by Louvain, and then mapping the pred labes and true labels, as the nodes are in the same order.\n",
    "\n",
    "We then look for where the predicted is the same as the ground truth, and use np.mean to get the accuracy.\n",
    "\n",
    "For the confusion matrix we construct the empty array and populate it by counting the number of times (true,pred) pair occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "louvain_labels = list(partition.values())\n",
    "\n",
    "#ground truth community labels as a list\n",
    "gt_labels = [G_reptile_attr.nodes[node_id][\"Class\"] for node_id in G_reptile_attr.nodes()]\n",
    "gt_labels = [labels if labels is not None else \"None\" for labels in gt_labels] # convert None to string(None)\n",
    "\n",
    "#mapping between pred labels and ground truth labels\n",
    "label_mapping = {}\n",
    "for louvain_label, gt_label in zip(louvain_labels, gt_labels):\n",
    "    if louvain_label not in label_mapping:\n",
    "        label_mapping[louvain_label] = gt_label\n",
    "\n",
    "\n",
    "louvain_labels_mapped = [label_mapping[label] for label in louvain_labels] #convert pred labels to ground truth labels\n",
    "\n",
    "accuracy = np.mean([1 if louvain == gt else 0 for louvain, gt in zip(louvain_labels_mapped,gt_labels)])\n",
    "print(f\"Accuracy of Louvain: {accuracy:.3f}\")\n",
    "\n",
    "labels = sorted(set(gt_labels + louvain_labels_mapped)) #Create a sorted list of unique labels\n",
    "num_classes = len(labels)\n",
    "\n",
    "cm = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "# Count the number of occurrences of each (true, pred) label pair\n",
    "for true, pred in zip(gt_labels, louvain_labels_mapped):\n",
    "    cm[labels.index(true)][labels.index(pred)] += 1\n",
    "\n",
    "confusion_matrix_df = pd.DataFrame(cm, index=unique_labels, columns=range(num_classes))\n",
    "print(confusion_matrix_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualise the graph with different splits we create n unique colors and add a color attribute to our graph. The first uses Louvain splits and the second uses attribute splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_communities = len(set(partition.values()))\n",
    "hue_start = 0.0\n",
    "saturation = 0.8\n",
    "value = 0.8\n",
    "\n",
    "colors = []\n",
    "for i in range(num_communities):\n",
    "    hue = hue_start + (i / num_communities)\n",
    "    r, g, b = colorsys.hsv_to_rgb(hue, saturation, value)\n",
    "    color_hex = \"#{:02x}{:02x}{:02x}\".format(int(r * 255), int(g * 255), int(b * 255))\n",
    "    colors.append(color_hex)\n",
    "\n",
    "community_numb = list(partition.values())\n",
    "for i, n in enumerate(G_reptile_attr.nodes()):\n",
    "    G_reptile.nodes[n]['color'] = colors[community_numb[i]]\n",
    "network, config = visualize(G_reptile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = []\n",
    "for i in range(len(unique_labels)):\n",
    "    hue = hue_start + (i / len(unique_labels))\n",
    "    r, g, b = colorsys.hsv_to_rgb(hue, saturation, value)\n",
    "    color_hex = \"#{:02x}{:02x}{:02x}\".format(int(r * 255), int(g * 255), int(b * 255))\n",
    "    colors.append(color_hex)\n",
    "\n",
    "print(f\"Amount of unique familys in dataset: {len(unique_labels):.0f}\")\n",
    "for i, n in enumerate(G_reptile_attr.nodes()):\n",
    "    G_reptile.nodes[n]['color'] = colors[unique_labels[G_reptile_attr.nodes[n][\"Class\"]]]\n",
    "\n",
    "network, config = visualize(G_reptile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text theory \n",
    "\n",
    "TF-IDF "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discussion. Think critically about your creation\n",
    "What went well?\n",
    "\n",
    "What is still missing? \n",
    "\n",
    "What could be improved? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
