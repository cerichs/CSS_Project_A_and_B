{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explainer Notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivation\n",
    "What is your dataset?\n",
    "\n",
    "Why did you choose this/these particular dataset(s)?\n",
    "\n",
    "What was your goal for the end user’s experience?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset consists of animal pages on Wikipedia, where we used wescraping methods and the WikiData API to collect a dataset.\n",
    "We choose this particular dataset because when looking at Wikipedia in general there is always a link to another page within a wikipedia page and then another link. This we thought would be interesting to look at in regards to the kingdom of animals. Because some groups of animals are more likely to mention each other in a wikipedia article. We first analyzed all of the approxomately 30k wikipedia pages and then narrowed down our network to only reptiles in order to narrow down the dataset.\n",
    "Our goal with the analysis of our Wikipedia animal dataset was to show the user and viewer how contected these animals actually are on wikipedia."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic stats\n",
    "Write about your choices in data cleaning and preprocessing\n",
    "\n",
    "Write a short section that discusses the dataset stats (here you can recycle the work you did for Project Assignment A)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that will be used in the analysis, has been gathered by using the WikiData API using their query builder. Due to the Wikipedia pages not having a category called “Animals” that allows you to gather all the animal pages, we will use an alternative which is querying on the Wikidata pages that have a “Animal Diversity Website (ADW) taxon id”. The query to gather the data is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "    SELECT DISTINCT ?item ?itemLabel WHERE {\n",
    "      SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE]\". }\n",
    "      {\n",
    "        SELECT DISTINCT ?item WHERE {\n",
    "          ?item p:P4024 ?statement0.\n",
    "          ?statement0 (ps:P4024) _:anyValueP4024.\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    '''\n",
    "query_reptile = '''\n",
    "    SELECT DISTINCT ?item ?itemLabel WHERE {\n",
    "      SERVICE wikibase:label { bd:serviceParam wikibase:language \"[AUTO_LANGUAGE]\". }\n",
    "      {\n",
    "        SELECT DISTINCT ?item WHERE {\n",
    "          ?item p:P4024 ?statement0.\n",
    "          ?statement0 (ps:P4024) _:anyValueP4024.\n",
    "          ?item p:P5473 ?statement1.\n",
    "          ?statement1 (ps:P5473) _:anyValueP5473.\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from get_links import links_on_page\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from netwulf import visualize\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first query just request all the pages where it has a Animal Diversity Website (ADW) taxon id, this query is made by their auto query builder.\n",
    "The second query request all the pages where it has a Animal Diversity Website (ADW) taxon id aswell as a Reptile Database ID.\n",
    "\n",
    "The get_wiki_links function takes n amount of wikidata ids, and in the query they are seperated with | as this allows us to query up to 50 ids at a time. We then loop over the response and check if it contains a link to an english wikipedia site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_links(item_ids):\n",
    "    url = f\"https://www.wikidata.org/w/api.php?action=wbgetentities&ids={'|'.join(item_ids)}&props=sitelinks/urls&format=json&sitefilter=enwiki\"\n",
    "    response = requests.get(url)\n",
    "    data = json.loads(response.text)\n",
    "    wiki_links = []\n",
    "    for item_id in item_ids:\n",
    "        wikipedia_url = data[\"entities\"][item_id][\"sitelinks\"]\n",
    "        if \"enwiki\" in wikipedia_url: # only save if the page has a reference to an english wikipage\n",
    "            link = wikipedia_url[\"enwiki\"][\"url\"]\n",
    "            wiki_links.append(link)\n",
    "    return wiki_links"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the actual quering, we start by getting all the wikidata pages that fit our query (query & query_reptile). We loop over 50 pages at a time, and get their wikipedia links with get_wiki_links, and write the results to a .txt file where each entry is seperated by a new line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://query.wikidata.org/sparql'\n",
    "r = requests.get(url, params = {'format': 'json', 'query': query_reptile})\n",
    "data = r.json()\n",
    "\n",
    "data = data[\"results\"][\"bindings\"]\n",
    "temp = []\n",
    "for entries in tqdm(range(0,len(data),50)):\n",
    "    if entries+50 < len(data):\n",
    "        sub_list = [ids[\"itemLabel\"][\"value\"] for ids in data[entries:(entries+50)]]\n",
    "        temp = temp + get_wiki_links(sub_list)\n",
    "    else:\n",
    "        sub_list = [ids[\"itemLabel\"][\"value\"] for ids in data[entries:(len(data))]]\n",
    "        temp = temp + get_wiki_links(sub_list)\n",
    "file = open('animal_links_reptile.txt','w')\n",
    "for item in temp:\n",
    "    file.write(item+\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for creating our nodes and edges. We open the .txt file from before, and prepare for the web scraping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist_weights = {}\n",
    "edgelist_weights_long = {}\n",
    "#animal_name = \"Elephant\"\n",
    "names = names_from_table() # Gets all the entries in the larger table on https://en.wikipedia.org/wiki/List_of_animal_names\n",
    "names_long = {}\n",
    "with open('data/animal_links_reptile.txt', 'r') as f:\n",
    "    entries = f.read().splitlines()\n",
    "for name in tqdm(entries):\n",
    "    name_temp = name.split(\"/\")[-1]\n",
    "    names_long[name_temp] = 0 # Saving all the entries in the txt file in a dict, for a fast comparisons (ie. only make pairs\n",
    "                                #  with animals and not wikipages for unrelated stuff)\n",
    "attributes_dict = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each wikipedia link in the .txt file, we get all the links on the page aswell as look for infoboxes using links_on_page. This function just uses the wikipedia api to get all the information on the page in json format, and then we find all href in the wikipedia page and save these.\n",
    "Then we look for a infobox, since there are 2 ways wikipedia makes these, we have to check if infobox is none, as this indicates the infobox is the other type.\n",
    "Then we go through the infobox (table) and extract predefined values we wanna use as attributes.\n",
    "In case there is no infobox on the page, ie. infobox is never updated from having None as values, we discard this page as it is most likely a redirect page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def links_on_page(animal_name=\"Elephant\"):\n",
    "    url = \"https://en.wikipedia.org/w/api.php?action=parse&page=\"+animal_name+\"&format=json\" # we can either put the title page or the URL version\n",
    "                                                                                             # of the title, ex. Malayan softshell turtle = Malayan_softshell_turtle\n",
    "    response = requests.get(url)\n",
    "    html = json.loads(response.content.decode('utf-8'))['parse']['text']['*'] # Default way the result comes in\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    links = soup.find_all('a', href=lambda href: href and href.startswith('/wiki/') and not href.endswith('.jpg') and not href.endswith('.png'))\n",
    "    # ^ sometimes the links link to jpg and png file, sort them out\n",
    "    result = []\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        title = link.get('title')\n",
    "        text = link.text\n",
    "        result.append(href)\n",
    "    info = {\"Class:\": None, \"Order:\":None, \"Superfamily:\":None, \"Family:\":None,\"Name:\": None} # Default dict allows us to see if its redirect later\n",
    "    infobox = soup.find('table', {'class': 'infobox biota biota-infobox'}) # One type of infoboxes wiki uses\n",
    "    if infobox is None:\n",
    "        infobox = soup.find('table', {'class': 'infobox biota'}) # Other type of infoboxes wiki uses\n",
    "    if infobox:\n",
    "        rows = infobox.find_all('tr') # Going through the rows in the infobox\n",
    "        for row in rows:\n",
    "            td = row.find_all('td')\n",
    "            if td: # Checking if empty\n",
    "                if td[0].text.strip() in [\"Class:\", \"Order:\", \"Superfamily:\", \"Family:\"]: # the info we want is stored in a row at a time\n",
    "                                                                                          # it has two columns first being Class, Order...\n",
    "                                                                                          # the other being the attributes we will save \n",
    "                    if td:\n",
    "                        info[td[0].text.strip()] = td[1].text.strip() # Making the category the key, and the attribute value the value\n",
    "        info[\"Name:\"] = animal_name # Updating from default \n",
    "\n",
    "    return result, info"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the graph we make is the one where we only make a edge a list of 224 animal names from https://en.wikipedia.org/wiki/List_of_animal_names, this wikipedia page contains two tables, one of them being one with the overall animal species (no subspecies).\n",
    "Due to it being a wikipedia page, there is a lot of references and stuff we dont want in our scraping which is removed with re.sub, where we have given it some predefined things to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names_from_table():\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_animal_names\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    html_content = response.content\n",
    "    \n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find table on the page\n",
    "    table = soup.find_all('table',{\"class\":\"wikitable\"})[1]\n",
    "    \n",
    "    # Find all the rows in the table\n",
    "    t_rows = table.find_all('tr')\n",
    "    \n",
    "    ths = t_rows[0].find_all('th')\n",
    "    \n",
    "    header = [th.text.replace(\"\\n\",\"\") for th in ths]\n",
    "    \n",
    "    rows = []\n",
    "    names = {}\n",
    "    for tr in t_rows[1:]:\n",
    "        tds = tr.find_all('td')\n",
    "        if tds:\n",
    "            links = tds[0].find_all('a')\n",
    "            if links:\n",
    "                row = [td.text.replace(\"\\n\",\"\") for td in tds]\n",
    "                rows.append(row)\n",
    "                cleaned_row = re.sub(r'\\(.*\\)|Also see.*|\\[\\d+\\]|See.*', '', row[0]) # The table is not clean, many unwanted formating we remove here\n",
    "                for link in links:\n",
    "                    link_href = link.get('href')\n",
    "                    if link_href.startswith('/wiki/'):\n",
    "                        names[link_href] = cleaned_row\n",
    "                        break\n",
    "    import pandas as pd\n",
    "    df_animals = pd.DataFrame(rows, columns=header)\n",
    "\n",
    "    return names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the actual scraping of websites, we loop over all the entries from our .txt file. We check if its redirect by looking at if infobox is None (its default scenario) if not, we add it to our edgelist. One of them to be used for directed and the other for undirected graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in tqdm(entries):\n",
    "    temp_string = name.split(\"/\")[-1]\n",
    "    result, info = links_on_page(animal_name=temp_string)\n",
    "    if info[\"Name:\"] is not None: # A way to remove wiki redirects from the final result, as redirects dont have the infoboxes we want\n",
    "        attributes_dict[info[\"Name:\"]] = info # Making nested dicts to quickly get attributes later\n",
    "        for entry in result:\n",
    "            if entry in names: # making one graph where we only make edges to the table from https://en.wikipedia.org/wiki/List_of_animal_names\n",
    "                pair = (\"/wiki/\"+temp_string,entry) # Making pairs to compare for the dict\n",
    "                pair_inverted = (entry,\"/wiki/\"+temp_string)\n",
    "                if pair in edgelist_weights:\n",
    "                    edgelist_weights[pair] += 1 # If the pair is already in the dict, the weight is increased\n",
    "                elif pair_inverted in edgelist_weights:\n",
    "                    edgelist_weights[pair_inverted] += 1 # If the inverted pair is already in the dict, the weight is increased\n",
    "                else:\n",
    "                    edgelist_weights[pair] = 1 # If the pair is not in the dict, the weight is 1\n",
    "            if entry.split(\"/\")[-1] in names_long: # making other graph where we make edges between entries from the .txt file\n",
    "                pair = (temp_string, entry.split(\"/\")[-1]) # Only taking the last part of the URL (the URL title of the page)\n",
    "                if pair in edgelist_weights_long:\n",
    "                    edgelist_weights_long[pair] += 1 # If the pair is already in the dict, the weight is increased\n",
    "                else:\n",
    "                    edgelist_weights_long[pair] = 1 # If the pair is not in the dict, the weight is 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid doing this over and over we dump the results as pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/data_plain_reptile_test.pickle', 'wb') as fp:\n",
    "    pickle.dump(edgelist_weights, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/data_plain_long_reptile_test.pickle', 'wb') as fp:\n",
    "    pickle.dump(edgelist_weights_long, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('data/Reptile_attributes.pickle', 'wb') as fp:\n",
    "    pickle.dump(attributes_dict, fp, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visulize them, they are first loaded and then we loop over all the entries and add them to an edgelist, and construct the graph with this edgelist.\n",
    "We then add all our attributes to the graph, and for sanity check we remove nodes than dont have attributes saved in our .pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/all_animal_to_all_animal.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "with open('data/animal_attributes.pickle', 'rb') as handle:\n",
    "    c = pickle.load(handle)\n",
    "\n",
    "values = list(b.values())\n",
    "plt.hist(values, bins=np.arange(max(values))-0.5, edgecolor='black')\n",
    "plt.yscale(\"log\")\n",
    "plt.xticks(range(1, max(values) + 1))\n",
    "plt.xlabel('Number of references')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "edgelist = [None]*len(b)\n",
    "for i,items in enumerate(b):\n",
    "    edgelist[i] = (items[0].replace(\"/wiki/\", \"\"),items[1].replace(\"/wiki/\", \"\"),int(b[items]))\n",
    "G = nx.DiGraph()\n",
    "\n",
    "G.add_weighted_edges_from(edgelist)\n",
    "print(G)\n",
    "\n",
    "to_remove =[]\n",
    "for Names in tqdm(G.nodes):\n",
    "    if Names in c:\n",
    "        G.nodes[Names]['Class'], G.nodes[Names]['Order'], G.nodes[Names]['Superfamily'], G.nodes[Names]['Family'], _ = c[Names].values()\n",
    "    else:\n",
    "        to_remove.append(Names) # Some nodes get added to graph even though they are redirects, the cause is known but no good way to handle it\n",
    "for names in to_remove:\n",
    "    G.remove_node(names)\n",
    "print(G)\n",
    "network, config = visualize(G)\n",
    "\n",
    "degree = []\n",
    "for node in G.nodes():\n",
    "    degree.append(G.degree(node))\n",
    "plt.hist(degree,bins=np.arange(max(values))-0.5, edgecolor='black')\n",
    "plt.title(\"Degree distribution\")\n",
    "plt.xlabel('Number of degrees')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tools, theory and analysis. Describe the process of theory to insight\n",
    "Describe which network science tools and data analysis strategies you’ve used, how those network science measures work, and why the tools you’ve chosen are right for the problem you’re solving.\n",
    "\n",
    "Talk about how you’ve worked with text, including regular expressions, unicode, etc.\n",
    "\n",
    "How did you use the tools to understand your dataset?\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network theory\n",
    "\n",
    "Fraction of edges\n",
    "\n",
    "Assortivity\n",
    "\n",
    "Modularity\n",
    "\n",
    "$M=\\sum_{c=1}^{n_c}\\left[\\frac{L_c}{L}-(\\frac{k_c}{2L})^2\\right]$ \n",
    "\n",
    "##### Louvain\n",
    "The Louvain algorithm is a community detection algorithm used to identify the communities or groups within a network.\n",
    "\n",
    "The algorithm works by optimizing a modularity function that measures the quality of the community structure. The modularity function quantifies the extent to which the number of edges within communities is higher than the expected number in a random network with the same degree sequence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text theory \n",
    "\n",
    "TF-IDF "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discussion. Think critically about your creation\n",
    "What went well?\n",
    "\n",
    "What is still missing? \n",
    "\n",
    "What could be improved? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
