{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from get_links import links_on_page\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import sklearn\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from animal_list import names_from_table\n",
    "from netwulf import visualize\n",
    "from wordcloud import WordCloud\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_names = names_from_table()\n",
    "animal_names_set = set(animal_names.values())\n",
    "# lower case all names\n",
    "animal_names_set = set([x.lower() for x in animal_names_set])\n",
    "\n",
    "# read txt file with pandas\n",
    "animal_df = pd.read_csv('data/animal_links.txt', header=None)\n",
    "animal_df.columns = ['page-name']\n",
    "\n",
    "# remove the first part of the url\n",
    "animal_df['page-name'] = animal_df['page-name'].str.replace('https://en.wikipedia.org', '', regex=False)\n",
    "animal_df[\"name\"] = animal_df[\"page-name\"].str.split(\"/\").str[-1]\n",
    "\n",
    "# lower case everything\n",
    "animal_df[\"name\"] = animal_df[\"name\"].str.lower()\n",
    "animal_df[\"page-name\"] = animal_df[\"page-name\"].str.lower()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page-name</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/wiki/scaly-crowned_babbler</td>\n",
       "      <td>scaly-crowned_babbler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/wiki/velvet-fronted_nuthatch</td>\n",
       "      <td>velvet-fronted_nuthatch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/wiki/mangrove_whistler</td>\n",
       "      <td>mangrove_whistler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/wiki/mees%27s_white-eye</td>\n",
       "      <td>mees%27s_white-eye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/wiki/ictiobus</td>\n",
       "      <td>ictiobus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31749</th>\n",
       "      <td>/wiki/eutropis_multicarinata</td>\n",
       "      <td>eutropis_multicarinata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31750</th>\n",
       "      <td>/wiki/gehyra_vorax</td>\n",
       "      <td>gehyra_vorax</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31751</th>\n",
       "      <td>/wiki/minervarya_andamanensis</td>\n",
       "      <td>minervarya_andamanensis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31752</th>\n",
       "      <td>/wiki/minervarya_greenii</td>\n",
       "      <td>minervarya_greenii</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31753</th>\n",
       "      <td>/wiki/zakerana_murthii</td>\n",
       "      <td>zakerana_murthii</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31754 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           page-name                     name\n",
       "0        /wiki/scaly-crowned_babbler    scaly-crowned_babbler\n",
       "1      /wiki/velvet-fronted_nuthatch  velvet-fronted_nuthatch\n",
       "2            /wiki/mangrove_whistler        mangrove_whistler\n",
       "3           /wiki/mees%27s_white-eye       mees%27s_white-eye\n",
       "4                     /wiki/ictiobus                 ictiobus\n",
       "...                              ...                      ...\n",
       "31749   /wiki/eutropis_multicarinata   eutropis_multicarinata\n",
       "31750             /wiki/gehyra_vorax             gehyra_vorax\n",
       "31751  /wiki/minervarya_andamanensis  minervarya_andamanensis\n",
       "31752       /wiki/minervarya_greenii       minervarya_greenii\n",
       "31753         /wiki/zakerana_murthii         zakerana_murthii\n",
       "\n",
       "[31754 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# animal_names\n",
    "animal_df\n",
    "\n",
    "# # load in reptile dataset just for testing purposes\n",
    "# with open('data/data_plain_reptile.pickle', 'rb') as handle:\n",
    "#     a = pickle.load(handle)\n",
    "# with open('data/data_plain_long_reptile.pickle', 'rb') as handle:\n",
    "#     b = pickle.load(handle)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function that will run on each row in the dataframe, which will take the page name and return all the readable text on the page\n",
    "def get_text(page_name):\n",
    "    url = 'https://en.wikipedia.org' + page_name\n",
    "    response = requests.get(url)\n",
    "    html_content = response.content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    # Find all the paragraphs in the body\n",
    "    paragraphs = soup.body.find_all('p')\n",
    "    # Extract the text from the paragraphs but remove \\n\n",
    "    # text = [p.text for p in paragraphs]\n",
    "    text = [str(p.text.replace('\\n', '')).strip() for p in paragraphs]\n",
    "    # Join the paragraphs together\n",
    "    joined_text = ' '.join(text)\n",
    "    # remove first space\n",
    "    joined_text = joined_text[1:]\n",
    "    return joined_text\n",
    "\n",
    "# function that can be passed to the tf-idf vectorizer that will preprocess the text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text\n",
    "\n",
    "# function we will apply to the tf-idf matrix to get the top n words\n",
    "def get_top_words(row, n=300):\n",
    "    return row.sort_values(ascending=False).head(n).to_dict()\n",
    "\n",
    "# generate wordcloud\n",
    "def generate_wordcloud(tf_idf:dict):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white')\n",
    "    wordcloud.generate_from_frequencies(tf_idf)\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.savefig('wordcloud.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will define a function to do all the steps above - tf-idf, get top words and return a dictionary with this information\n",
    "\n",
    "def get_information_dict(filepath:str, load=True, save=False, preprocess=True) -> dict:\n",
    "    \"\"\"\n",
    "    Function that will read a parquet file, or csv file, or json file, or pickle file, and return a dictionary with the dataframe, top words and tf-idf matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str\n",
    "        The name of the file to read\n",
    "    load : bool, optional\n",
    "        Whether to load the file or not. The default is True. Can only be used if the file is already made, have to have run with save as True before\n",
    "    save : bool, optional\n",
    "        Whether to save the file or not. The default is False\n",
    "    preprocess : bool, optional\n",
    "        Whether to preprocess the text or not. The default is True\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary with the dataframe, top words and tf-idf matrix\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    print(f\"Reading {filepath}\")\n",
    "    # get the filepath before the extension\n",
    "    filepath_without_extension = filepath.split('.')[0]\n",
    "    if filepath.endswith('.parquet'):\n",
    "        df = pd.read_parquet(filepath)\n",
    "    elif filepath.endswith('.csv'):\n",
    "        df = pd.read_csv(filepath)\n",
    "    elif filepath.endswith('.json'):\n",
    "        df = pd.read_json(filepath)\n",
    "    elif filepath.endswith('long_reptile.pickle'):\n",
    "        with open('data/data_plain_long_reptile.pickle', 'rb') as handle:\n",
    "            a = pickle.load(handle)\n",
    "        a = {str(k[0]).lower(): v for k, v in a.items()}\n",
    "        df = animal_df[animal_df['name'].isin(a.keys())].reset_index(drop=True)\n",
    "    elif filepath.endswith('reptile.pickle'):\n",
    "        with open('data/data_plain_reptile.pickle', 'rb') as handle:\n",
    "            a = pickle.load(handle)\n",
    "        a = {str(k[0]).lower(): v for k, v in a.items()}\n",
    "        df = animal_df[animal_df['page-name'].isin(a.keys())].reset_index(drop=True)\n",
    "    \n",
    "    if load:\n",
    "        try:\n",
    "            df = pd.read_parquet(filepath_without_extension + '.parquet')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {filepath} not found\")\n",
    "            return\n",
    "    else:\n",
    "        # get the text from the page\n",
    "        print(f\"Getting text\")\n",
    "        df['text'] = df['page-name'].progress_apply(get_text)\n",
    "\n",
    "    if save:\n",
    "        # save as parquet file\n",
    "        df.to_parquet(filepath_without_extension + '.parquet')\n",
    "    \n",
    "    if preprocess:\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', dtype=np.float32, preprocessor=preprocess_text)\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(stop_words='english', dtype=np.float32)\n",
    "    \n",
    "    tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.todense(), columns=feature_names)\n",
    "    \n",
    "    print(f\"Getting top words\")\n",
    "    top_words = tfidf_df.progress_apply(get_top_words, axis=1)\n",
    "    top_words.index = df['name']\n",
    "\n",
    "    if save:\n",
    "        # save as a json file\n",
    "        top_words.to_json(filepath_without_extension + '_top_words.json')\n",
    "    \n",
    "    return {'df': df, 'top_words': top_words, 'tfidf_df': tfidf_df}\n",
    "    \n",
    "    \n",
    "        \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data/data_plain_long_reptile.pickle\n",
      "Getting top words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 764/764 [00:01<00:00, 382.88it/s]\n"
     ]
    }
   ],
   "source": [
    "data_dict_reptile_long = get_information_dict('data/data_plain_long_reptile.pickle', load=True, save=True, preprocess=True)\n",
    "data_dict_reptile = get_information_dict('data/data_plain_reptile.pickle', load=True, save=True, preprocess=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to check the score of the words in the top words for each animal\n",
    "# We will do this by getting the tf-idf score for each word in the top words for each animal\n",
    "\n",
    "\n",
    "for name, top_words in data_dict_reptile['top_words'].items():\n",
    "    tw_keys = set([k for k, value in top_words.items() if value > 0]) # get the keys of the top words that have a score > 0\n",
    "\n",
    "    links = list(animal_names_set.intersection(tw_keys))\n",
    "    if links:\n",
    "        link_dict = {name: {k: top_words[k]} for k in links}\n",
    "        \n",
    "        # add to the df\n",
    "        # \n",
    "        data_dict_reptile['df'].loc[data_dict_reptile['df']['name'] == name, 'links'] = str(link_dict[name])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page-name</th>\n",
       "      <th>name</th>\n",
       "      <th>text</th>\n",
       "      <th>links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/wiki/chironius_laurenti</td>\n",
       "      <td>chironius_laurenti</td>\n",
       "      <td>Chironius laurenti is a species of nonvenomous...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/wiki/dasia_olivacea</td>\n",
       "      <td>dasia_olivacea</td>\n",
       "      <td>Dasia olivacea, the olive dasia or olive tree ...</td>\n",
       "      <td>{'skink': 0.05847395211458206}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/wiki/alabama_map_turtle</td>\n",
       "      <td>alabama_map_turtle</td>\n",
       "      <td>The Alabama map turtle (Graptemys pulchra) is ...</td>\n",
       "      <td>{'turtle': 0.27575668692588806}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/wiki/geoscincus</td>\n",
       "      <td>geoscincus</td>\n",
       "      <td>Geoscincus is a monotypic genus of skinks: the...</td>\n",
       "      <td>{'skink': 0.11515277624130249}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/wiki/gonyosoma_oxycephalum</td>\n",
       "      <td>gonyosoma_oxycephalum</td>\n",
       "      <td>Elaphe oxycephala (Boie, 1827) Gonyosoma oxyce...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4339</th>\n",
       "      <td>/wiki/eugongylus_albofasciolatus</td>\n",
       "      <td>eugongylus_albofasciolatus</td>\n",
       "      <td>The white-striped cape skink or barred shark s...</td>\n",
       "      <td>{'skink': 0.3205573856830597}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4340</th>\n",
       "      <td>/wiki/eumecia_johnstoni</td>\n",
       "      <td>eumecia_johnstoni</td>\n",
       "      <td>Eumecia johnstoni is a species of skink found ...</td>\n",
       "      <td>{'skink': 0.22512587904930115}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4341</th>\n",
       "      <td>/wiki/giant_plated_lizard</td>\n",
       "      <td>giant_plated_lizard</td>\n",
       "      <td>The giant plated lizard (Matobosaurus validus)...</td>\n",
       "      <td>{'lizard': 0.22714518010616302}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4342</th>\n",
       "      <td>/wiki/eutropis_multicarinata</td>\n",
       "      <td>eutropis_multicarinata</td>\n",
       "      <td>Eutropis multicarinata is a species of skink f...</td>\n",
       "      <td>{'skink': 0.2291412651538849}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4343</th>\n",
       "      <td>/wiki/gehyra_vorax</td>\n",
       "      <td>gehyra_vorax</td>\n",
       "      <td>Gehyra vorax, also known as the voracious four...</td>\n",
       "      <td>{'gecko': 0.20271454751491547}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4344 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             page-name                        name  \\\n",
       "0             /wiki/chironius_laurenti          chironius_laurenti   \n",
       "1                 /wiki/dasia_olivacea              dasia_olivacea   \n",
       "2             /wiki/alabama_map_turtle          alabama_map_turtle   \n",
       "3                     /wiki/geoscincus                  geoscincus   \n",
       "4          /wiki/gonyosoma_oxycephalum       gonyosoma_oxycephalum   \n",
       "...                                ...                         ...   \n",
       "4339  /wiki/eugongylus_albofasciolatus  eugongylus_albofasciolatus   \n",
       "4340           /wiki/eumecia_johnstoni           eumecia_johnstoni   \n",
       "4341         /wiki/giant_plated_lizard         giant_plated_lizard   \n",
       "4342      /wiki/eutropis_multicarinata      eutropis_multicarinata   \n",
       "4343                /wiki/gehyra_vorax                gehyra_vorax   \n",
       "\n",
       "                                                   text  \\\n",
       "0     Chironius laurenti is a species of nonvenomous...   \n",
       "1     Dasia olivacea, the olive dasia or olive tree ...   \n",
       "2     The Alabama map turtle (Graptemys pulchra) is ...   \n",
       "3     Geoscincus is a monotypic genus of skinks: the...   \n",
       "4     Elaphe oxycephala (Boie, 1827) Gonyosoma oxyce...   \n",
       "...                                                 ...   \n",
       "4339  The white-striped cape skink or barred shark s...   \n",
       "4340  Eumecia johnstoni is a species of skink found ...   \n",
       "4341  The giant plated lizard (Matobosaurus validus)...   \n",
       "4342  Eutropis multicarinata is a species of skink f...   \n",
       "4343  Gehyra vorax, also known as the voracious four...   \n",
       "\n",
       "                                links  \n",
       "0                                 NaN  \n",
       "1      {'skink': 0.05847395211458206}  \n",
       "2     {'turtle': 0.27575668692588806}  \n",
       "3      {'skink': 0.11515277624130249}  \n",
       "4                                 NaN  \n",
       "...                               ...  \n",
       "4339    {'skink': 0.3205573856830597}  \n",
       "4340   {'skink': 0.22512587904930115}  \n",
       "4341  {'lizard': 0.22714518010616302}  \n",
       "4342    {'skink': 0.2291412651538849}  \n",
       "4343   {'gecko': 0.20271454751491547}  \n",
       "\n",
       "[4344 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict_reptile[\"df\"]\n",
    "# data_dict_reptile[\"top_words\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alternatvie text](figures/network.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above the graph of the network can be seen for the file data_plain_reptile which tracks how many times all the pages refer back to the 224 long list of animals (main animals). We see a clear clustering around the nodes: Lizard, Skink, Turtle, Snake and Gecko to name the largest. Let us investigate if we can see a link for some of the nodes in these clusters through our text analysis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look at the animal laudakia_dayana which can faintly be seen in the lizard cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Laudakia_dayana'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\otto\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\otto\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\otto\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Laudakia_dayana'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\otto\\OneDrive - Danmarks Tekniske Universitet\\DTU\\Kurser\\02467 Computational Social Science\\CSS_Project_A\\text_analysis.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/otto/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/DTU/Kurser/02467%20Computational%20Social%20Science/CSS_Project_A/text_analysis.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# sort top words by name\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/otto/OneDrive%20-%20Danmarks%20Tekniske%20Universitet/DTU/Kurser/02467%20Computational%20Social%20Science/CSS_Project_A/text_analysis.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data_dict_reptile[\u001b[39m\"\u001b[39;49m\u001b[39mtop_words\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39mLaudakia_dayana\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n",
      "File \u001b[1;32mc:\\Users\\otto\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:958\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    955\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[0;32m    957\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> 958\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[0;32m    960\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    961\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    962\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\otto\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:1069\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[0;32m   1068\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1069\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[0;32m   1070\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[1;32mc:\\Users\\otto\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Laudakia_dayana'"
     ]
    }
   ],
   "source": [
    "# sort top words by name\n",
    "data_dict_reptile[\"top_words\"][\"Laudakia_dayana\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
